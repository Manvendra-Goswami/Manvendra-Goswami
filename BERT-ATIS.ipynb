{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPTK/nG/eZ/7TKtGpQXOwYO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manvendra-Goswami/Manvendra-Goswami/blob/main/BERT-ATIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3I3JLOI6uzg",
        "outputId": "304496cf-5d13-4dd4-9d97-3042c8669b3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "# verify GPU availability\n",
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install\n",
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "metadata": {
        "id": "E3yKvS9AFj50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# BERT imports\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "id": "9B85gienAKXD",
        "outputId": "654bd448-82d6-4260-ad66-98a99a37da67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.5)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.63.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.37)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.10.0.2)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.5.2)\n",
            "Requirement already satisfied: botocore<1.25.0,>=1.24.37 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.24.37)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.37->boto3->pytorch-pretrained-bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.37->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.37->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "DATA_DIR=\".\"\n",
        "\n",
        "# load Pickle file \n",
        "#def load_ds(fname=os.path.join(DATA_DIR,'/atis.train.pkl'), verbose=True):\n",
        "def load_ds(fname='atis.train.pkl', verbose=True):\n",
        "    with open(fname, 'rb') as stream:\n",
        "        ds,dicts = pickle.load(stream)\n",
        "    if verbose:\n",
        "      print('Done  loading: ', fname)\n",
        "      print('      samples: {:4d}'.format(len(ds['query'])))\n",
        "      print('   vocab_size: {:4d}'.format(len(dicts['token_ids'])))\n",
        "      print('   slot count: {:4d}'.format(len(dicts['slot_ids'])))\n",
        "      print(' intent count: {:4d}'.format(len(dicts['intent_ids'])))\n",
        "    return ds,dicts\n",
        "  \n",
        "# convert Pickle file to arrays\n",
        "def load_atis(filename, add_start_end_token=False, verbose=True):\n",
        "    train_ds, dicts = load_ds(os.path.join(DATA_DIR,filename), verbose)\n",
        "    t2i, s2i, in2i = map(dicts.get, ['token_ids', 'slot_ids','intent_ids'])\n",
        "    i2t, i2s, i2in = map(lambda d: {d[k]:k for k in d.keys()}, [t2i,s2i,in2i])\n",
        "    query, slots, intent =  map(train_ds.get, ['query', 'slot_labels', 'intent_labels'])\n",
        "\n",
        "    if add_start_end_token:\n",
        "        i2s[178] = 'BOS'\n",
        "        i2s[179] = 'EOS'\n",
        "        s2i['BOS'] = 178\n",
        "        s2i['EOS'] = 179\n",
        "\n",
        "    input_tensor = []\n",
        "    target_tensor = []\n",
        "    query_data = []\n",
        "    intent_data = []\n",
        "    slot_data = []\n",
        "    to_show = np.random.randint(0, len(query)-1, 5)\n",
        "    for i in range(len(query)):\n",
        "        input_tensor.append(query[i])\n",
        "        slot_text = []\n",
        "        slot_vector = []\n",
        "        for j in range(len(query[i])):\n",
        "            slot_text.append(i2s[slots[i][j]])\n",
        "            slot_vector.append(slots[i][j])\n",
        "        if add_start_end_token:\n",
        "            slot_text[0] = 'BOS'\n",
        "            slot_vector[0] = 178\n",
        "            slot_text[-1] = 'EOS'\n",
        "            slot_vector[-1]= 179\n",
        "        target_tensor.append(slot_vector)\n",
        "        q = ' '.join(map(i2t.get, query[i]))\n",
        "        query_data.append(q.replace('BOS', '').replace('EOS',''))\n",
        "        intent_data.append(i2in[intent[i][0]])\n",
        "        slot = ' '.join(slot_text)\n",
        "        slot_data.append(slot[1:-1])\n",
        "        if i in to_show and verbose:\n",
        "          print('Query text:', q)\n",
        "          print('Query vector: ', query[i])\n",
        "          print('Intent label: ', i2in[intent[i][0]])\n",
        "          print('Slot text: ', slot)\n",
        "          print('Slot vector: ', slot_vector)\n",
        "          print('*'*74)\n",
        "    query_data = np.array(query_data)\n",
        "    intent_data = np.array(intent_data)\n",
        "    slot_data = np.array(slot_data)\n",
        "    intent_data_label = np.array(intent).flatten()\n",
        "    return t2i, s2i, in2i, i2t, i2s, i2in, input_tensor, target_tensor, query_data, intent_data, intent_data_label, slot_data\n",
        "\n",
        "# load ATIS training dataset\n",
        "t2i_train, s2i_train, in2i_train, i2t_train, i2s_train, i2in_train, \\\n",
        "input_tensor_train, target_tensor_train, \\\n",
        "query_data_train, intent_data_train, intent_data_label_train, slot_data_train = load_atis('atis.train.pkl')\n",
        "\n",
        "# load ATIS testing dataset\n",
        "t2i_test, s2i_test, in2i_test, i2t_test, i2s_test, i2in_test, \\\n",
        "input_tensor_test, target_tensor_test, \\\n",
        "query_data_test, intent_data_test, intent_data_label_test, slot_data_test = load_atis('atis.test.pkl')"
      ],
      "metadata": {
        "id": "PBkKrLaLA4wD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ed8c71-4363-47f4-aee8-9afc7efc81bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done  loading:  ./atis.train.pkl\n",
            "      samples: 4978\n",
            "   vocab_size:  943\n",
            "   slot count:  129\n",
            " intent count:   26\n",
            "Query text: BOS which are the flights from denver to baltimore or washington dc EOS\n",
            "Query vector:  [178 920 228 827 429 444 351 851 247 662 905 344 179]\n",
            "Intent label:  flight\n",
            "Slot text:  O O O O O O B-fromloc.city_name O B-toloc.city_name B-or B-toloc.city_name B-toloc.state_code O\n",
            "Slot vector:  [128, 128, 128, 128, 128, 128, 48, 128, 78, 56, 78, 80, 128]\n",
            "**************************************************************************\n",
            "Query text: BOS what flights from atlanta to washington EOS\n",
            "Query vector:  [178 916 429 444 242 851 905 179]\n",
            "Intent label:  flight\n",
            "Slot text:  O O O O B-fromloc.city_name O B-toloc.city_name O\n",
            "Slot vector:  [128, 128, 128, 128, 48, 128, 78, 128]\n",
            "**************************************************************************\n",
            "Query text: BOS please list all the takeoffs and landings for general mitchell international EOS\n",
            "Query vector:  [178 688 549 207 827 813 215 524 435 445 600 496 179]\n",
            "Intent label:  flight\n",
            "Slot text:  O O O O O O O O O B-airport_name I-airport_name I-airport_name O\n",
            "Slot vector:  [128, 128, 128, 128, 128, 128, 128, 128, 128, 4, 84, 84, 128]\n",
            "**************************************************************************\n",
            "Query text: BOS does continental fly from boston to san francisco EOS\n",
            "Query vector:  [178 376 325 431 444 266 851 739 440 179]\n",
            "Intent label:  flight\n",
            "Slot text:  O O B-airline_name O O B-fromloc.city_name O B-toloc.city_name I-toloc.city_name O\n",
            "Slot vector:  [128, 128, 2, 128, 128, 48, 128, 78, 125, 128]\n",
            "**************************************************************************\n",
            "Query text: BOS show flights from pittsburgh to oakland connecting through denver EOS\n",
            "Query vector:  [178 770 429 444 682 851 644 320 844 351 179]\n",
            "Intent label:  flight\n",
            "Slot text:  O O O O B-fromloc.city_name O B-toloc.city_name B-connect O B-stoploc.city_name O\n",
            "Slot vector:  [128, 128, 128, 128, 48, 128, 78, 20, 128, 71, 128]\n",
            "**************************************************************************\n",
            "Done  loading:  ./atis.test.pkl\n",
            "      samples:  893\n",
            "   vocab_size:  943\n",
            "   slot count:  129\n",
            " intent count:   26\n",
            "Query text: BOS i need flight and fare information for thursday departing prior to 9 am from oakland going to salt lake city EOS\n",
            "Query vector:  [178 479 617 428 215 414 492 435 845 353 696 851 172 210 444 644 452 851\n",
            " 736 521 301 179]\n",
            "Intent label:  flight+airfare\n",
            "Slot text:  O O O O O O O O B-depart_date.day_name O B-depart_time.time_relative I-depart_time.time_relative B-depart_time.time I-depart_time.time O B-fromloc.city_name O O B-toloc.city_name I-toloc.city_name I-toloc.city_name O\n",
            "Slot vector:  [128, 128, 128, 128, 128, 128, 128, 128, 26, 128, 36, 101, 35, 100, 128, 48, 128, 128, 78, 125, 125, 128]\n",
            "**************************************************************************\n",
            "Query text: BOS list california nevada arizona airports EOS\n",
            "Query vector:  [178 549 279 618 230 204 179]\n",
            "Intent label:  airport\n",
            "Slot text:  O O B-state_name B-state_name B-state_name O O\n",
            "Slot vector:  [128, 128, 68, 68, 68, 128, 128]\n",
            "**************************************************************************\n",
            "Query text: BOS show me the cheapest one way flights from montreal to orlando EOS\n",
            "Query vector:  [178 770 581 827 296 656 906 429 444 604 851 667 179]\n",
            "Intent label:  flight\n",
            "Slot text:  O O O O B-cost_relative B-round_trip I-round_trip O O B-fromloc.city_name O B-toloc.city_name O\n",
            "Slot vector:  [128, 128, 128, 128, 21, 66, 119, 128, 128, 48, 128, 78, 128]\n",
            "**************************************************************************\n",
            "Query text: BOS give me the cheapest round trip flights from indianapolis to orlando around december twenty fifth EOS\n",
            "Query vector:  [178 449 581 827 296 730 870 429 444 489 851 667 231 348 881 421 179]\n",
            "Intent label:  flight\n",
            "Slot text:  O O O O B-cost_relative B-round_trip I-round_trip O O B-fromloc.city_name O B-toloc.city_name B-depart_time.time_relative B-depart_date.month_name B-depart_date.day_number I-depart_date.day_number O\n",
            "Slot vector:  [128, 128, 128, 128, 21, 66, 119, 128, 128, 48, 128, 78, 36, 28, 27, 95, 128]\n",
            "**************************************************************************\n",
            "Query text: BOS please list the flights from los angeles to charlotte EOS\n",
            "Query vector:  [178 688 549 827 429 444 563 216 851 294 179]\n",
            "Intent label:  flight\n",
            "Slot text:  O O O O O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name O\n",
            "Slot vector:  [128, 128, 128, 128, 128, 128, 48, 110, 128, 78, 128]\n",
            "**************************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the train file from your local drive\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "# Upload data from Google Drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/drive')\n",
        "\n",
        "#\n",
        "# queries are stored in the variable query_data_train\n",
        "# correct intent labels are stored in the variable labels\n",
        "#\n",
        "\n",
        "# add special tokens for BERT to work properly\n",
        "sentences = [\"[CLS] \" + query + \" [SEP]\" for query in query_data_train]\n",
        "print(sentences[0])\n",
        "\n",
        "# Tokenize with BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuwjAYvrGUWv",
        "outputId": "18a3f902-fd87-4dd4-9ab3-422a489d7690"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]  i want to fly from boston at 838 am and arrive in denver at 1110 in the morning  [SEP]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1873497.33B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'i', 'want', 'to', 'fly', 'from', 'boston', 'at', '83', '##8', 'am', 'and', 'arrive', 'in', 'denver', 'at', '111', '##0', 'in', 'the', 'morning', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the maximum sequence length. \n",
        "MAX_LEN = 128\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "metadata": {
        "id": "mgwZkvijHILc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ],
      "metadata": {
        "id": "OW-3HWdVHbLN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(intent_data_label_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR6L_HoPLOgU",
        "outputId": "bd308b65-8602-49d4-9927-6b14022d785f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14 14 19 ...  6 14 14]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, intent_data_label_train, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.1)\n",
        "                                             \n",
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "# Select a batch size for training. \n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader \n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "6GcxZJMVHi54"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=nb_labels)\n",
        "model.cuda()\n",
        "\n",
        "# BERT model summary\n",
        "BertForSequenceClassification(\n",
        "  (bert): BertModel(\n",
        "    (embeddings): BertEmbeddings(\n",
        "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
        "      (position_embeddings): Embedding(512, 768)\n",
        "      (token_type_embeddings): Embedding(2, 768)\n",
        "      (LayerNorm): BertLayerNorm()\n",
        "      (dropout): Dropout(p=0.1)\n",
        "    )\n",
        "    (encoder): BertEncoder(\n",
        "      (layer): ModuleList(\n",
        "        (0): BertLayer(\n",
        "          (attention): BertAttention(\n",
        "            (self): BertSelfAttention(\n",
        "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (dropout): Dropout(p=0.1)\n",
        "            )\n",
        "            (output): BertSelfOutput(\n",
        "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (LayerNorm): BertLayerNorm()\n",
        "              (dropout): Dropout(p=0.1)\n",
        "            )\n",
        "          )\n",
        "          (intermediate): BertIntermediate(\n",
        "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "          )\n",
        "          (output): BertOutput(\n",
        "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "            (LayerNorm): BertLayerNorm()\n",
        "            (dropout): Dropout(p=0.1)\n",
        "          )\n",
        "        )\n",
        "        '\n",
        "        '\n",
        "        '\n",
        "      )\n",
        "    )\n",
        "    (pooler): BertPooler(\n",
        "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "      (activation): Tanh()\n",
        "    )\n",
        "  )\n",
        "  (dropout): Dropout(p=0.1)\n",
        "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
        ")"
      ],
      "metadata": {
        "id": "iUEUFWSxNS2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Jbjld4u2HaQZ"
      }
    }
  ]
}